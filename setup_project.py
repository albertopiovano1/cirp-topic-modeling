#!/usr/bin/env python3
"""
Setup script for CIRP Bibliometric Analysis project
Creates necessary directory structure and placeholder files
"""

import os
from pathlib import Path

def create_project_structure():
    """Create all necessary directories and placeholder files"""
    
    print("="*80)
    print("CIRP BIBLIOMETRIC ANALYSIS - PROJECT SETUP")
    print("="*80)
    
    # Define directory structure
    directories = [
        "data",
        "notebooks",
        "results",
        "figures",
        "models"
    ]
    
    # Create directories
    print("\nüìÅ Creating directory structure...")
    for dir_name in directories:
        dir_path = Path(dir_name)
        dir_path.mkdir(exist_ok=True)
        
        # Create .gitkeep to preserve empty directories in git
        gitkeep = dir_path / ".gitkeep"
        if not gitkeep.exists():
            gitkeep.touch()
        
        print(f"   ‚úÖ {dir_name}/")
    
    # Create README files for each directory
    readme_contents = {
        "data": """# Data Directory

Place your input dataset here:
- **Required file:** `CIRP_researchonly.csv`
- **Required columns:** Title, Abstract, Year, Document Type

The dataset should contain research papers from CIRP publications.
""",
        "notebooks": """# Notebooks Directory

Place Jupyter notebooks for exploratory analysis here:
- Data exploration
- Preliminary topic analysis
- Figure generation for paper
- Custom visualizations
""",
        "results": """# Results Directory

This directory will contain CSV exports generated by the pipeline:
- `00_Analysis_Report.txt`
- `01_Full_Document_Assignments.csv`
- `02_Top_Papers_Per_Topic.csv`
- `03_Topic_Metadata_Keywords.csv`
- `04_Summary_Statistics.csv`

These files are automatically generated - do not edit manually.
""",
        "figures": """# Figures Directory

This directory will contain interactive HTML visualizations:
- Intertopic distance maps
- Topic word scores
- Hierarchical clustering
- Similarity heatmaps
- Temporal evolution plots

Open HTML files in a web browser to interact with visualizations.
""",
        "models": """# Models Directory

Optional: Save trained BERTopic models here for reuse:
- Model checkpoints
- Embedding caches
- Custom representations

*Note: Model files can be large (1-2 GB). Consider using .gitignore.*
"""
    }
    
    print("\nüìÑ Creating README files...")
    for dir_name, content in readme_contents.items():
        readme_path = Path(dir_name) / "README.md"
        if not readme_path.exists():
            with open(readme_path, 'w') as f:
                f.write(content)
            print(f"   ‚úÖ {dir_name}/README.md")
    
    # Check for main script
    print("\nüîç Checking for main script...")
    if Path("cirp_bertopic_v5_0.py").exists():
        print("   ‚úÖ cirp_bertopic_v5_0.py found")
    else:
        print("   ‚ö†Ô∏è  cirp_bertopic_v5_0.py not found")
    
    # Check for requirements
    print("\nüì¶ Checking for requirements.txt...")
    if Path("requirements.txt").exists():
        print("   ‚úÖ requirements.txt found")
    else:
        print("   ‚ö†Ô∏è  requirements.txt not found")
    
    print("\n" + "="*80)
    print("‚úÖ PROJECT SETUP COMPLETE")
    print("="*80)
    print("\nNext steps:")
    print("1. Install dependencies: pip install -r requirements.txt")
    print("2. Download spaCy model: python -m spacy download en_core_web_sm")
    print("3. Place your dataset in data/CIRP_researchonly.csv")
    print("4. Run pipeline: python cirp_bertopic_v5_0.py")
    print("\n" + "="*80 + "\n")


if __name__ == "__main__":
    create_project_structure()
